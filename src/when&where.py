# -*- coding: utf-8 -*-
"""When&Where.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B-iMTTKW-fEse22AJWrYU2EaRBjQ48tr
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import folium
from folium.plugins import HeatMap
import glob
import os
import kagglehub

path = kagglehub.dataset_download("currie32/crimes-in-chicago")

all_files = glob.glob(os.path.join(path, "*.csv"))
df_list = [pd.read_csv(f, index_col=0, on_bad_lines='skip').sample(frac=0.1) for f in all_files]
df = pd.concat(df_list, axis=0, ignore_index=True)

df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y %I:%M:%S %p')
df['Hour'] = df['Date'].dt.hour
df['DayOfWeek'] = df['Date'].dt.day_name()
df

plt.figure(figsize=(12,6))
sns.countplot(data = df, x='Hour', palette='viridis')
plt.title('Crime Incidents by Hour of day (Chicago)')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Incidents')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

df.isna().count()

df_geo=df.dropna(subset=['Latitude', 'Longitude'])
df_geo.isna().count()

m = folium.Map(location=[41.8781, -87.6298], zoom_start=12)
m

heat_data = df_geo[['Latitude', 'Longitude']].sample(5000).values.tolist()
HeatMap(heat_data).add_to(m)

m.save('Chicago_crime_heatmap.html')
print("Heatmap saved as 'chicago_crime_heatmap.html'")

friday_10pm = df[(df['DayOfWeek'] == 'Friday')& (df['Hour'] == 22)]
sector_counts = friday_10pm['District'].value_counts().head(5)
print("Top 5 districts for Crime on Friday at 10:00 PM")
print(sector_counts)

plt.figure(figsize=(10,5))
sector_counts.plot(kind='bar', color='salmon')
plt.title('Incidents on Friday at 10:00 PM by district')
plt.ylabel('Number of Crimes')
plt.show()

heatmap_data = df.pivot_table(values= 'ID', index='DayOfWeek', columns='Hour', aggfunc='count')
days =['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
heatmap_data = heatmap_data.reindex(days)

plt.figure(figsize=(15,8))
sns.heatmap(heatmap_data, cmap='YlOrRd', annot=False)
plt.title('The Police Battle Plan: Crime Density by Day & Hour')
plt.show()

def get_deployment_plan(day, hour):
  subset = df[(df['DayOfWeek'] == day) & (df['Hour'] == hour)]
  top_districts = subset['District'].value_counts().head(3)
  main_crime = subset['Primary Type'].mode()[0]
  return {
      "Time Slot": f"{day} at {hour}:00",
      "Priority Districts": top_districts.index.tolist(),
      "Main Crime": main_crime,
      "Recommanded Action": f"Deploy {main_crime} Task Force to Districts {top_districts.index.tolist()}"
  }

print(get_deployment_plan("Friday", 22))

df['Is_Weekend'] = df['Date'].dt.dayofweek.apply(lambda x: 1 if x>= 5 else 0)

def get_part_of_day(hour):
  if 5 <= hour < 12:
    return 'Morning'
  elif 12<= hour < 17:
    return 'Afternoon'
  elif 17 <= hour < 21:
    return 'Evening'
  else:
    return 'Night'

df['Part_of_Day'] = df['Date'].dt.hour.apply(get_part_of_day)
df

def get_season(month):
  if month in [12, 1, 2]:
    return 'Winter'
  elif month in [3, 4, 5]:
    return 'Spring'
  elif month in [6, 7, 8]:
    return 'Summer'
  else:
    return 'Autumn'
df['Season'] = df['Date'].dt.month.apply(get_season)
df

from pandas.tseries.holiday import USFederalHolidayCalendar as calendar

cal = calendar()
holidays = cal.holidays(start=df['Date'].min(), end=df['Date'].max())
df['Is_Holiday'] = df['Date'].dt.date.astype('datetime64[ns]').isin(holidays).astype(int)

df[df['Is_Holiday'] == 1]['Primary Type'].value_counts()

import geopandas as gpd
from shapely.geometry import Point

df_geo = df.dropna(subset=['Latitude', 'Longitude']).copy()
df_geo

geometry = [Point(xy) for xy in zip (df_geo['Longitude'], df_geo['Latitude'])]
gdf = gpd.GeoDataFrame(df_geo, geometry=geometry, crs='EPSG:4326')
gdf

gdf = gdf.to_crs(epsg=3435)
gdf

city_center_raw = Point(-87.6325, 41.8832)
center_point = gpd.GeoSeries([city_center_raw], crs="EPSG:4326").to_crs(epsg=3435).iloc[0]

gdf['Distance_to_Center'] = gdf.geometry.distance(center_point)
gdf['Distance_KM']=gdf['Distance_to_Center']-0.0003048

gdf

violent_crimes = ['HOMICIDE', 'ROBBERY', 'ASSUALT', 'BATTERY','CRIM SEXUAL ASSAULT']
gdf['Is_Violent'] = gdf['Primary Type'].apply(lambda x: 'Violent' if x in violent_crimes else 'Non-Violent')

gdf['Is_Violent'].value_counts()

analysis = gdf.groupby('Is_Violent')['Distance_KM'].mean()
print("Average Distance from City Center:")
print(analysis)

plt.figure(figsize=(10,6))
sns.kdeplot(data=gdf, x='Distance_KM', hue='Is_Violent', fill=True)
plt.title('Density of Crimes by Distance from City Center')
plt.xlabel('Distance (km)')
plt.show()

import folium
from folium.plugins import MarkerCluster

city_hall = [41.8832, -876325]
m=folium.Map(location=city_hall, zoom_start=11, tiles='CartoDB positron')

distances = [2000, 5000, 10000]
colors = ['green', 'orange', 'red']

for dist, color in zip(distances, colors):
  folium.Circle(
      location=city_hall,
      radius=dist,
      color=color,
      fill=True,
      fill_color=0.1,
      popup=f'{dist/1000}km from Center'
  ).add_to(m)

sample_df = gdf.sample(1000)

for idx, row in sample_df.iterrows():
  color = 'red' if row['Is_Violent'] == 'Violent' else 'blue'
  folium.CircleMarker(
      location=[row['Latitude'], row['Longitude']],
      radius=3,
      color=color,
      fill = True,
      fill_color=color,
      fill_opacity=0.7,
      popup=f"Type: {row['Primary Type']}<br>Dist: {row['Distance_KM']:.2f} km"
  ).add_to(m)

folium.Marker(
    location=city_hall,
    icon=folium.Icon(color='black', icon='star'),
    popup='CITY CENTER/COMMAND POST'
).add_to(m)
m.save("spatial_analysis.html")

from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

features = ['Hour', 'Is_Weekend', 'Part_of_Day', 'Season', 'Is_Holiday', 'District', 'Distance_KM']
target = 'Primary Type'
df_ml = gdf[features +[target]+ ['Date']].dropna()

encoders = {}
for col in ['Part_of_Day', 'Season', 'Primary Type']:
  le = LabelEncoder()
  df_ml[col] = le.fit_transform(df_ml[col])
  encoders[col] = le

df_ml

df_ml['Year'] = df_ml['Date'].dt.year
train_df = df_ml[(df_ml['Year']==2001) | (df_ml['Year']==2016)]
test_df = df_ml[df_ml['Year']==2016]

X_train = train_df[features]
y_train = train_df[target]
X_test = test_df[features]
y_test = test_df[target]
print(f"Training rows:{len(train_df)}, Testing rows:{len(test_df)}")

df['Year'].value_counts()

rf =RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)
print("Training Model...")
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
print("Model Trained.")

from sklearn.metrics import classification_report

present_classes = np.unique(np.concatenate((y_test, y_pred)))
target_names = encoders['Primary Type'].inverse_transform(present_classes)
print(classification_report(y_test, y_pred, labels=present_classes, target_names=target_names))

importances = pd.DataFrame({'feature': features, 'importance':rf.feature_importances_})
print(importances.sort_values('importance', ascending = False))

gdf['Distance_KM'] = gdf['Distance_to_Center'] * 0.0003048
gdf_clean = gdf.dropna(subset=['Distance_KM']).copy()
max_dist = gdf_clean['Distance_KM'].max()
print(f"Farthest crime is {max_dist:.2f} km away.")
gdf_clean['Distance_Bucket'] = pd.cut(gdf_clean['Distance_KM'], bins=[0,2,5,10,20, max_dist],labels=['0-2km (Core)', '2-5km (Inner)', '5-10km (Mid)', '10-20km (Outer)','20km+ (Fringe)'], include_lowest=True)
for bucket in gdf_clean['Distance_Bucket'].cat.categories:
  print(f"\n-- ZONE: {bucket} KM from Center")
  zone_data = gdf_clean[gdf_clean['Distance_Bucket'] == bucket]
  if not zone_data.empty:
    top_crimes = zone_data['Primary Type'].value_counts(normalize=True).head(3)*100
    for crime, percent in top_crimes.items():
      print(f"{crime}: {percent:.2f}%")
  else:
    print("No crimes in this zone.")

from math import radians, cos, sin, asin, sqrt

def haversine(lon1, lat1, lon2, lat2):
  lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
  dlon = lon2 - lon1
  dlat = lat2 - lat1
  a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
  c = 2 * asin(sqrt(a))
  r = 6371
  return c*r

target_lat, target_lon = 41.8837, -87.6324
gdf['Latitude'] = pd.to_numeric(gdf['Latitude'], errors='coerce')
gdf['Longitude'] = pd.to_numeric(gdf['Longitude'], errors='coerce')
gdf = gdf.dropna(subset=['Latitude', 'Longitude'])

gdf['Dist_to_Target'] = gdf.apply(lambda row: haversine(row['Longitude'], row['Latitude'], target_lon, target_lat), axis=1)
print(gdf.groupby('Primary Type')['Dist_to_Target'].mean().sort_values())
gdf['Is_Night'] = gdf['Hour'].apply(lambda x: 1 if x >= 20 or x <=5 else 0)

theft_night_ratio = gdf[gdf['Primary Type'] == 'THEFT']['Is_Night'].mean()
print(f"Percentage of thefts at night : {theft_night_ratio: .2%}")

from imblearn.over_sampling import SMOTE
from collections import Counter

print("Before SMOTE:", Counter(y_train))

smote = SMOTE(random_state=42, k_neighbors=1)
x_train_res, y_train_res = smote.fit_resample(X_train, y_train)
print("After SMOTE:", Counter(y_train_res))

rf_balanced = RandomForestClassifier(n_estimators=100, max_depth=12, random_state=42)
rf_balanced.fit(x_train_res, y_train_res)

y_pred_balanced = rf_balanced.predict(X_test)
print(classification_report(y_test, y_pred_balanced, labels=present_classes, target_names=target_names))

from sklearn.cluster import DBSCAN

violent_pts = gdf[gdf['Is_Violent'] == 'Violent'].copy()
coords = np.vstack([violent_pts.geometry.x, violent_pts.geometry.y]).T
coords.shape

coords

db = DBSCAN(eps=500, min_samples=15).fit(coords)

violent_pts['Cluster'] = db.labels_

num_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)
print(f"Discovered {num_clusters} high-density violent crime hotspot")

m_hotspots = folium.Map(location=[41.8781, -87.6298], zoom_start=11, titles='CartoDB dark_matter')
hotspots_only = violent_pts[violent_pts['Cluster'] != -1]

for cluster_id in hotspots_only['Cluster'].unique():
  cluster_data = hotspots_only[hotspots_only['Cluster'] == cluster_id]

  center_lat = cluster_data['Latitude'].mean()
  center_lon = cluster_data['Longitude'].mean()

  folium.Circle(
      location=[center_lat, center_lon],
      radius=200,
      color='red',
      fill=True,
      popup=f"Hotspot #{cluster_id}<br>Incident: {len(cluster_data)}"
  ).add_to(m_hotspots)
m_hotspots.save("chicago_tactical_hotspots.html")

severity_weights = {
    'HOMICIDE': 10,
    'CRIM SEXUAL ASSAULT': 10,
    'KIDNAPPING': 9,
    'ROBBERY': 7,
    'WEAPONS VIOLATION': 7,
    'ASSAULT': 5,
    'BATTERY': 5,
    'BURGLARY': 4,
    'MOTOR VEHICLE THEFT': 4,
    'NARCOTICS': 3,
    'THEFT': 2,
    'CRIMINAL DAMAGE': 2,
    'DECEPTIVE PRACTICE': 1,
    'PUBLIC PEACE VIOLATION': 1
}
severity_weights

gdf['Risk_Weight'] = gdf['Primary Type'].apply(lambda x: severity_weights.get(x,1))

district_stats = gdf.groupby('District').agg({
    'ID': 'count',
    'Risk_Weight': 'sum'
}).rename(columns={'ID': 'Crime_Volume', 'Risk_Weight': 'Total_Risk_Score'})

district_stats['Avg_Severity'] = district_stats['Total_Risk_Score']/district_stats['Crime_Volume']

district_stats = district_stats.sort_values(by='Total_Risk_Score', ascending=False)

district_stats

top_by_volume = district_stats.sort_values('Crime_Volume', ascending=False).head(3)
top_by_risk = district_stats.sort_values('Total_Risk_Score', ascending=False).head(3)
print("Top 3 districts by volume (Traditional View):")
print(top_by_volume[['Crime_Volume', 'Total_Risk_Score']])

print("\nTop 3 Districts by Risk:")
print(top_by_risk[['Total_Risk_Score', 'Crime_Volume', 'Avg_Severity']])

plt.figure(figsize=(12, 8))
sns.scatterplot(data=district_stats, x='Crime_Volume', y = 'Total_Risk_Score', size='Avg_Severity', sizes=(100,1000), hue='Avg_Severity', palette='Reds')
for district_id, row in district_stats.iterrows():
  plt.text(row['Crime_Volume']+5, row['Total_Risk_Score']+5, str(district_id), fontsize=9)
plt.title('Crime Volume vs. Risk Score')
plt.xlabel('Crime Volume')
plt.ylabel('Total Risk Score')
plt.grid(True, alpha=0.3)
plt.show()